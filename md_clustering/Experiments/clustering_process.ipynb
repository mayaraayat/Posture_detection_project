{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.9.0)\n",
      "Collecting pytorch-lightning\n",
      "  Using cached pytorch_lightning-2.1.3-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2023.3.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (4.8.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.8.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->pytorch-lightning) (3.0.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.12.0->pytorch-lightning) (3.7.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.12.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.12.0->pytorch-lightning) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.2.1)\n",
      "Using cached pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
      "Installing collected packages: pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 1.9.0\n",
      "    Uninstalling pytorch-lightning-1.9.0:\n",
      "      Successfully uninstalled pytorch-lightning-1.9.0\n",
      "Successfully installed pytorch-lightning-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform K-means:\n",
    "If you get an error about utils or multimarginal_OT change '../../' to '../' or vice versa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LazyTensor' from 'ot.utils' (/usr/users/detectionpositionassise/ayat_may/.local/lib/python3.8/site-packages/ot/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkmeans_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m perform_kmeans_clustering\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clusters\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_data\u001b[39m(file_path):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/Posture_detection/brouillon/md_clustering/Experiments/../utils/clustering_utils.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultimarginal_OT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolving_MM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m solve_multimarginal_optimal_transport\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultimarginal_OT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcost_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasserstein_cost_matrix\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m~/Posture_detection/brouillon/md_clustering/Experiments/../../multimarginal_OT/cost_matrix.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdictionary_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbarycenters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasserstein_barycenter_with_cost\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwasserstein_cost_matrix\u001b[39m(source_distribution, target_distribution):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Computes the Wasserstein cost matrix between two distributions.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        list: Cost matrix for the multimarginal optimal transport problem .\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Posture_detection/brouillon/md_clustering/Experiments/../../dictionary_learning/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Posture_detection/brouillon/md_clustering/Experiments/../../dictionary_learning/utils.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Module with utility functions.\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mot\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ot/__init__.py:23\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Author: Remi Flamary <remi.flamary@unice.fr>\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         Nicolas Courty <ncourty@irisa.fr>\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# All submodules and packages\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lp\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bregman\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ot/bregman/__init__.py:34\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_barycenter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (barycenter,\n\u001b[1;32m     25\u001b[0m                           barycenter_sinkhorn,\n\u001b[1;32m     26\u001b[0m                           free_support_sinkhorn_barycenter,\n\u001b[1;32m     27\u001b[0m                           barycenter_stabilized,\n\u001b[1;32m     28\u001b[0m                           barycenter_debiased,\n\u001b[1;32m     29\u001b[0m                           jcpot_barycenter)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (convolutional_barycenter2d,\n\u001b[1;32m     32\u001b[0m                              convolutional_barycenter2d_debiased)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_empirical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (empirical_sinkhorn,\n\u001b[1;32m     35\u001b[0m                          empirical_sinkhorn2,\n\u001b[1;32m     36\u001b[0m                          empirical_sinkhorn_divergence)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_screenkhorn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (screenkhorn)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dictionary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (unmix)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ot/bregman/_empirical.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Author: Remi Flamary <remi.flamary@unice.fr>\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         Kilian Fatras <kilian.fatras@irisa.fr>\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         Quang Huy Tran <quang-huy.tran@univ-ubs.fr>\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# License: MIT License\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dist, list_to_array, unif, LazyTensor\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_backend\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sinkhorn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sinkhorn, sinkhorn2\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LazyTensor' from 'ot.utils' (/usr/users/detectionpositionassise/ayat_may/.local/lib/python3.8/site-packages/ot/utils.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('../')\n",
    "import json\n",
    "import numpy as np\n",
    "from utils.kmeans_utils import perform_kmeans_clustering\n",
    "from utils.clustering_utils import clusters\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def extract_pressure_matrices(json_data):\n",
    "    pressure_matrices = []\n",
    "    for entry in json_data['pressureData']:\n",
    "        pressure_matrix = entry[\"pressureMatrix\"]\n",
    "        pressure_matrices.append({\"pressureMatrix\": pressure_matrix})\n",
    "    return pressure_matrices\n",
    "\n",
    "def extract_features_from_pressure_matrices(pressure_matrices):\n",
    "    return [np.array(item[\"pressureMatrix\"]).flatten() for item in pressure_matrices]\n",
    "\n",
    "def combine_features(features_posture, features_continuous):\n",
    "    return features_posture + features_continuous\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load continuous sitting data\n",
    "    continuous_data = load_json_data('Posture_Data/Data/Mayara/Continuous Data/SensingMatData_240112_154605.json')\n",
    "    continuous_1=load_json_data('Posture_Data/Data/Aaron/SensingMatData_231126_230808.json')\n",
    "    #print(continuous_data)\n",
    "    \n",
    "    # Extract features from the data\n",
    "    pressure_matrice_continuous = extract_pressure_matrices(continuous_data)\n",
    "    pressure_matrice_domain1=extract_pressure_matrices(continuous_1)\n",
    "    \n",
    "    \n",
    "    features_continuous = extract_features_from_pressure_matrices(pressure_matrice_continuous)\n",
    "    features_1=extract_features_from_pressure_matrices(pressure_matrice_domain1)\n",
    "    print(features_continuous)\n",
    "\n",
    "\n",
    "\n",
    "    num_clusters = 7  \n",
    "\n",
    "\n",
    "    # Perform k-means clustering for continuous data of the reference subject\n",
    "    cluster_labels_continuous, _ = perform_kmeans_clustering(np.array(features_continuous).reshape(-1,1), num_clusters)\n",
    "    cluster_labels_continuous1,_=perform_kmeans_clustering(np.array(features_1).reshape(-1,1), num_clusters)\n",
    "   \n",
    "    # Create cluster objects for each domain\n",
    "    continuous_domain = clusters(features_continuous, cluster_labels_continuous, num_clusters)\n",
    "    domain_1= clusters(features_1,cluster_labels_continuous1,num_clusters)\n",
    "    \n",
    "    # Cluster data for each domain\n",
    "    continuous_domain.cluster_data()\n",
    "    domain_1.cluster_data()\n",
    "    \n",
    "    # Save the results\n",
    "    np.save('Posture_Data\\Results\\KMeans\\Clusters.npy', cluster_labels_continuous)\n",
    "    mapped_labels_continuous = domain_1.clusters_mapping(continuous_domain.cluster_tensors)\n",
    "    np.save('Posture_Data\\Results\\KMeans\\MappedLabels_Continuous.npy', mapped_labels_continuous)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the atoms, we need pytorch 1.9 or to change the environment. We add the following cell to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning==1.9 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (2.1.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (6.0)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9) (2023.12.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (1.3.0)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (4.8.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.4.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pytorch-lightning==1.9) (0.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9) (2.27.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9) (3.8.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=17.1->pytorch-lightning==1.9) (3.0.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.10.0->pytorch-lightning==1.9) (3.7.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.10.0->pytorch-lightning==1.9) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.10.0->pytorch-lightning==1.9) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.10.0->pytorch-lightning==1.9) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.57.0->pytorch-lightning==1.9) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch>=1.10.0->pytorch-lightning==1.9) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nom\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch>=1.10.0->pytorch-lightning==1.9) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning==1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful machine or a GPU is needed for the atoms initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fit - Xs shapes: [torch.Size([29952, 1]), torch.Size([248320, 1])]\n",
      "Before fit - Xs shapes: [torch.Size([29952, 7]), torch.Size([248320, 7])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                 | Params\n",
      "-------------------------------------------------\n",
      "0 | loss_fn | JointWassersteinLoss | 0     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fit - Xs shapes: [torch.Size([29952, 1]), torch.Size([248320, 1])]\n",
      "Before fit - Ys shapes: [torch.Size([29952, 7]), torch.Size([248320, 7])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ae9e3b7b8d4d1c8523ee85b2d98369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fit - Xs shapes: [torch.Size([29952, 1]), torch.Size([248320, 1])]\n",
      "After fit - Ys shapes: [torch.Size([29952, 7]), torch.Size([248320, 7])]\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "|        Iteration        |          Loss           |          δLoss          |      Elapsed Time       |\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 59501445120 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m         np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myatom_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), y_value)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 72\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore fit - Xs shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, [y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Ys])\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Ensure n_samples is not larger than the size of the smallest dataset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#n_samples = min(n_samples, min(len(Xs), len(Ys[0]), len(Ys[1])))\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Compute the barycenters\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m atoms \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_barycenters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter_dil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mϵ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mη_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Getting initialized atoms\u001b[39;00m\n\u001b[0;32m     76\u001b[0m XP \u001b[38;5;241m=\u001b[39m [xatom[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m xatom \u001b[38;5;129;01min\u001b[39;00m atoms]\n",
      "File \u001b[1;32mc:\\Users\\NOM\\OneDrive - CentraleSupelec\\Bureau\\studies\\2A\\IA\\MultiDomainClustering-pole_ia\\Posture_detection\\md_clustering\\Experiments\\../..\\dictionary_learning\\weighted_barycenters.py:60\u001b[0m, in \u001b[0;36mcompute_barycenters\u001b[1;34m(Xs, Ys, n_samples, batch_size, num_iter_dil, n_classes, ε, η_A, lr, num_iter_max)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_atoms):\n\u001b[0;32m     59\u001b[0m     w_i \u001b[38;5;241m=\u001b[39m wbr\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39msqueeze()[i]\n\u001b[1;32m---> 60\u001b[0m     X_atom, Y_atom \u001b[38;5;241m=\u001b[39m \u001b[43mwasserstein_barycenter_with_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXs\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mϵ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mϵ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mα\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mβ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iter_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpropagate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mpenalize_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     atom_results\u001b[38;5;241m.\u001b[39mappend((X_atom, Y_atom))\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m atom_results\n",
      "File \u001b[1;32mc:\\Users\\NOM\\OneDrive - CentraleSupelec\\Bureau\\studies\\2A\\IA\\MultiDomainClustering-pole_ia\\Posture_detection\\md_clustering\\Experiments\\../..\\dictionary_learning\\barycenters.py:125\u001b[0m, in \u001b[0;36mwasserstein_barycenter_with_cost\u001b[1;34m(XP, YP, n_samples, ε, α, β, num_iter_max, num_iter_sinkhorn, τ, verbose, inner_verbose, initialization, propagate_labels, penalize_labels, device, log, covariance_type, label_metric)\u001b[0m\n\u001b[0;32m    122\u001b[0m C, π \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(XP)):\n\u001b[1;32m--> 125\u001b[0m     C_k \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    126\u001b[0m     _β \u001b[38;5;241m=\u001b[39m β \u001b[38;5;28;01mif\u001b[39;00m β \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m C_k\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m penalize_labels:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\functional.py:1315\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1313\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 59501445120 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dictionary_learning.weighted_barycenters import compute_barycenters\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def extract_pressure_matrices(json_data):\n",
    "    pressure_matrices = []\n",
    "    for entry in json_data['pressureData']:\n",
    "        pressure_matrix = entry[\"pressureMatrix\"]\n",
    "        pressure_matrices.append({\"pressureMatrix\": pressure_matrix})\n",
    "    return pressure_matrices\n",
    "\n",
    "def extract_features_from_pressure_matrices(pressure_matrices):\n",
    "    flattened_data = [np.array(item[\"pressureMatrix\"]).flatten() for item in pressure_matrices]\n",
    "    return np.concatenate(flattened_data, axis=0)\n",
    "\n",
    "def combine_features(features_posture, features_continuous):\n",
    "    return [features_posture , features_continuous]\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load continuous sitting data\n",
    "    continuous_data = load_json_data('Posture_Data/Data/Mayara/Continuous Data/SensingMatData_240112_154605.json')\n",
    "    continuous_1=load_json_data('Posture_Data/Data/Aaron/SensingMatData_231126_230808.json')\n",
    "    \n",
    "    pressure_matrices_continuous = extract_pressure_matrices(continuous_data)\n",
    "    pressure_1=extract_pressure_matrices(continuous_1)\n",
    "    \n",
    "    features_continuous = extract_features_from_pressure_matrices(pressure_matrices_continuous)\n",
    "    features_1=extract_features_from_pressure_matrices(pressure_1)\n",
    "    \n",
    "    \n",
    "    # Combine features if necessary\n",
    "    Y1 = np.load('Posture_Data\\Results\\KMeans\\Clusters_M_A.npy', allow_pickle=True)\n",
    "    Y2 = np.load('Posture_Data\\Results\\KMeans\\MappedLabels_Continuous_M_A.npy', allow_pickle=True)\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    n_classes = 7\n",
    "    n_samples = 100\n",
    "    batch_size = 64\n",
    "    ϵ = 0.01\n",
    "    η_A = 0.0\n",
    "    lr = 1e-1\n",
    "    num_iter_max = 20\n",
    "    num_iter_dil = 100\n",
    "    # Prepare data for the barycenter computation\n",
    "    Ys = [torch.nn.functional.one_hot(torch.from_numpy(Y1).long(), num_classes=7).float(),\n",
    "        torch.nn.functional.one_hot(torch.from_numpy(Y2).long(), num_classes=7).float()]\n",
    "    \n",
    "    l=[y.shape for y in Ys]\n",
    "    features_continuous_tensor = torch.from_numpy(features_continuous).view(l[0][0],-1)\n",
    "    features_1_tensor = torch.from_numpy(features_1).view(l[1][0],-1)\n",
    "\n",
    "\n",
    "    Xs = [features_continuous_tensor, features_1_tensor]\n",
    "    print(\"Before fit - Xs shapes:\", [x.shape for x in Xs])\n",
    "    print(\"Before fit - Xs shapes:\", [y.shape for y in Ys])\n",
    "    # Ensure n_samples is not larger than the size of the smallest dataset\n",
    "    #n_samples = min(n_samples, min(len(Xs), len(Ys[0]), len(Ys[1])))\n",
    "\n",
    "    # Compute the barycenters\n",
    "    atoms = compute_barycenters(Xs, Ys, n_samples, batch_size, num_iter_dil,\n",
    "                                n_classes, ϵ, η_A, lr, num_iter_max)\n",
    "    \n",
    "    # Getting initialized atoms\n",
    "    XP = [xatom[0] for xatom in atoms]\n",
    "    YP = [yatom[1] for yatom in atoms]\n",
    "    \n",
    "    print(\"Before converting types - XP and YP dtypes:\", XP[0].dtype, YP[0].dtype)\n",
    "\n",
    "   \n",
    "    XP = [x.to(torch.float32) for x in XP]\n",
    "    YP = [y.to(torch.float32) for y in YP]\n",
    "\n",
    "    \n",
    "    print(\"After converting types - XP and YP dtypes:\", XP[0].dtype, YP[0].dtype)\n",
    "\n",
    "    # Create the Results/Atoms directory if it doesn't exist\n",
    "    results_directory = \"Posture_Data/Results/Atoms\"\n",
    "    os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "    # Save atoms supports as NumPy files\n",
    "    for i, x_value in enumerate(XP):\n",
    "        np.save(os.path.join(results_directory, f'xatom_{i}.npy'), x_value)\n",
    "\n",
    "    for i, y_value in enumerate(YP):\n",
    "        np.save(os.path.join(results_directory, f'yatom_{i}.npy'), y_value)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.clustering_utils import clusters\n",
    "from dictionary_learning.DaDiL_clustering import *\n",
    "# Now you can use a simple import statement\n",
    "\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def extract_pressure_matrices(json_data):\n",
    "    pressure_matrices = []\n",
    "    for entry in json_data['pressureData']:\n",
    "        pressure_matrix = entry[\"pressureMatrix\"]\n",
    "        pressure_matrices.append({\"pressureMatrix\": pressure_matrix})\n",
    "    return pressure_matrices\n",
    "\n",
    "def extract_features_from_pressure_matrices(pressure_matrices):\n",
    "    return [np.array(item[\"pressureMatrix\"]).flatten() for item in pressure_matrices]\n",
    "\n",
    "def combine_features(features_posture, features_continuous):\n",
    "    return features_posture + features_continuous\n",
    "\n",
    "def main():\n",
    "    # Assuming posture_data is a list of JSON file paths\n",
    "    posture_data = [f'Posture_Data/Data/Subject1/Postures/Posture {i}.json' for i in range(1,8)]\n",
    "\n",
    "    # Now iterate over each file path, load JSON data, and extract pressure matrices\n",
    "    pressure_matrices_posture = []\n",
    "    for file_path in posture_data:\n",
    "        posture_entry = load_json_data(file_path)\n",
    "        pressure_matrices_posture.extend(extract_pressure_matrices(posture_entry))\n",
    "\n",
    "     # Load continuous sitting data\n",
    "    continuous_data = load_json_data('Posture_Data/Data/Mayara/Continuous Data/SensingMatData_240112_154605.json')\n",
    "    continuous_1=load_json_data('Posture_Data/Data/Aaron/SensingMatData_231126_230808.json')\n",
    "    \n",
    "    pressure_matrices_continuous = extract_pressure_matrices(continuous_data)\n",
    "    pressure_1=extract_pressure_matrices(continuous_1)\n",
    "    \n",
    "    features_continuous = extract_features_from_pressure_matrices(pressure_matrices_continuous)\n",
    "    features_1=extract_features_from_pressure_matrices(pressure_1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Y1 = np.load('Posture_Data\\Results\\KMeans\\Clusters_M_A.npy', allow_pickle=True)\n",
    "    Y2 = np.load('Posture_Data\\Results\\KMeans\\MappedLabels_Continuous_M_A.npy', allow_pickle=True)\n",
    "    \n",
    "    \n",
    "    # Prepare data for the barycenter computation\n",
    "    Ys = [torch.nn.functional.one_hot(torch.from_numpy(Y1).long(), num_classes=7).float(),\n",
    "        torch.nn.functional.one_hot(torch.from_numpy(Y2).long(), num_classes=7).float()]\n",
    "    \n",
    "    l=[y.shape for y in Ys]\n",
    "    features_continuous_tensor = torch.from_numpy(features_continuous).view(l[0][0],-1)\n",
    "    features_1_tensor = torch.from_numpy(features_1).view(l[1][0],-1)\n",
    "\n",
    "\n",
    "    features = [features_continuous_tensor, features_1_tensor]\n",
    "    Xs = features\n",
    "    print(\"Before fit - Xs shapes:\", [x.shape for x in Xs])\n",
    "    print(\"Before fit - Xs shapes:\", [y.shape for y in Ys])\n",
    "    # Ensure n_samples is not larger than the size of the smallest dataset\n",
    "    #n_samples = min(n_samples, min(len(Xs), len(Ys[0]), len(Ys[1])))\n",
    "\n",
    "\n",
    "    # Load XP and YP NumPy files\n",
    "    XP = []\n",
    "    YP = []\n",
    "    for i in range(len(features)):\n",
    "        x_file_path = os.path.join(\n",
    "            'Results/Atoms', f'xatom_{i}.npy')\n",
    "        y_file_path = os.path.join(\n",
    "            'Results/Atoms', f'yatom_{i}.npy')\n",
    "\n",
    "        # Load XP\n",
    "        loaded_x = np.load(x_file_path)\n",
    "        XP.append(torch.tensor(loaded_x))\n",
    "\n",
    "        # Load YP\n",
    "        loaded_y = np.load(y_file_path)\n",
    "        YP.append(torch.tensor(loaded_y))\n",
    "\n",
    "    # Define hyperparameters\n",
    "    n_classes = 7\n",
    "    n_samples = 3000\n",
    "    batch_size = 128\n",
    "    n_components = 3\n",
    "    n_datasets = 3\n",
    "    reg = 0.0\n",
    "    reg_labels = 0.0\n",
    "    num_iter_max = 100\n",
    "\n",
    "\n",
    "    # Perform the DaDiL clustering\n",
    "    cluster_labels = dadil_clustering(\n",
    "        Xs, Ys, XP, YP, n_samples, n_components, reg, reg_labels, batch_size, n_classes, num_iter_max)\n",
    "\n",
    "    domain_1 = clusters(features[0], cluster_labels[0], n_classes)\n",
    "    domain_2 = clusters(features[1], cluster_labels[1], n_classes)\n",
    "\n",
    "\n",
    "    # Cluster data for each domain\n",
    "\n",
    "    domain_1.cluster_data()\n",
    "    domain_2.cluster_data()\n",
    "\n",
    "\n",
    "    np.save('Results/DaDiL/MappedLabels_Posture.npy',\n",
    "            cluster_labels[0])\n",
    "\n",
    "    mapped_labels_domain_2 = domain_2.clusters_mapping(\n",
    "        domain_1.cluster_tensors)\n",
    "    np.save('Results/DaDiL/MappedLabels_Continuous.npy',\n",
    "            mapped_labels_domain_2)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
